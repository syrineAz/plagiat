# -*- coding: utf-8 -*-
"""Project-PLagiat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQa6BoGpHFUVSRPncXhMo4vU8-DQFALW
"""


from datasets import load_dataset
dataset = load_dataset("paws", "labeled_final")

dataset

# ==============================
# IMPORTS
# ==============================
import streamlit as st
import re
import nltk
import faiss
import numpy as np
import pdfplumber
import docx

from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize

# ==============================
# INITIALISATION NLTK
# ==============================
nltk.download("punkt")
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# ==============================
# STREAMLIT CONFIG
# ==============================
st.set_page_config(
    page_title="Plagiarism Detection",
    layout="wide"
)

st.title("ğŸ” Plagiarism Detection Application")
st.write("Dataset: **PAWS (Hugging Face)**")

# ==============================
# TEXT PREPROCESSING
# ==============================
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", " ", text)
    text = " ".join([w for w in text.split() if w not in stop_words])
    return text

def split_sentences(text):
    return sent_tokenize(text)

# ==============================
# FILE TEXT EXTRACTION
# ==============================
def extract_text(uploaded_file):
    if uploaded_file.type == "text/plain":
        return uploaded_file.read().decode("utf-8", errors="ignore")

    elif uploaded_file.type == "application/pdf":
        text = ""
        with pdfplumber.open(uploaded_file) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
        return text

    elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = docx.Document(uploaded_file)
        return "\n".join([p.text for p in doc.paragraphs])

    return ""

# ==============================
# LOAD DATASET (PAWS)
# ==============================
@st.cache_resource
def load_reference_data():
    dataset = load_dataset("paws", "labeled_final")
    reference_texts = dataset["train"]["sentence1"]
    cleaned_refs = [clean_text(t) for t in reference_texts]

    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    embeddings = model.encode(cleaned_refs, show_progress_bar=True)

    faiss.normalize_L2(embeddings)
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)

    return model, index, reference_texts

model, index, reference_texts = load_reference_data()

# ==============================
# PLAGIARISM DETECTION
# ==============================
def detect_plagiarism(text, threshold):
    sentences = split_sentences(text)
    results = []

    for sent in sentences:
        cleaned = clean_text(sent)
        if len(cleaned.strip()) == 0:
            continue

        emb = model.encode([cleaned])
        faiss.normalize_L2(emb)

        scores, ids = index.search(emb, 1)
        similarity = scores[0][0]

        if similarity >= threshold:
            results.append({
                "sentence": sent,
                "similarity": similarity,
                "reference": reference_texts[ids[0][0]]
            })

    return results

# ==============================
# STREAMLIT UI
# ==============================
uploaded_file = st.file_uploader(
    "ğŸ“¤ Upload a document",
    type=["txt", "pdf", "docx"]
)

threshold = st.slider(
    "ğŸ”§ Similarity threshold",
    min_value=0.5,
    max_value=0.95,
    value=0.8,
    step=0.05
)

if uploaded_file:
    user_text = extract_text(uploaded_file)

    if st.button("ğŸš€ Test Plagiarism"):
        with st.spinner("Analyzing document..."):
            results = detect_plagiarism(user_text, threshold)

        st.subheader("ğŸ“Š Results")

        if results:
            st.error("âš ï¸ Plagiarism detected")
            for r in results:
                st.markdown("---")
                st.write(f"**Sentence:** {r['sentence']}")
                st.write(f"**Similarity:** `{r['similarity']:.2f}`")
                st.write(f"**Matched reference:** {r['reference']}")
        else:
            st.success("âœ… No plagiarism detected")

